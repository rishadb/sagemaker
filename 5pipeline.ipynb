{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b4bb64-57cd-4a68-990a-494ed92cb10c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build a SageMaker Pipeline to train and deploy a BERT-Based text classifier\n",
    "* Define and run a pipeline using a directed acyclic graph (DAG) with specific pipeline parameters and model hyper-parameters\n",
    "* Define a processing step that cleans, balances, transforms, and splits our dataset into train, validation, and test dataset\n",
    "* Define a training step that trains a model using the train and validation datasets\n",
    "* Define a processing step that evaluates the trained model's performance on the test dataset\n",
    "* Define a register model step that creates a model package from the trained model\n",
    "* Define a conditional step that checks the model's performance and conditionally registers the model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bbc375-ae90-4909-bcc3-97af573f65f7",
   "metadata": {},
   "source": [
    "**Terminology**\n",
    "\n",
    "This notebook focuses on the following features of Amazon SageMaker Pipelines:\n",
    "\n",
    "* **Pipelines** - a directed acyclic graph (DAG) of steps and conditions to orchestrate SageMaker jobs and resource creation\n",
    "* **Processing job steps** - a simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model explainability\n",
    "* **Training job steps** - an iterative process that teaches a model to make predictions on new data by presenting examples from a training dataset\n",
    "* **Conditional step execution** - provides conditional execution of branches in a pipeline\n",
    "* **Registering models** - register a model in a model registry to create a deployable models in Amazon SageMaker\n",
    "* **Parameterized pipeline executions** - allows pipeline executions to vary by supplied parameters\n",
    "* **Model endpoint** - hosts the model as a REST endpoint to serve predictions from new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac49ee8-e28a-46c8-95a7-e78905bdb6a9",
   "metadata": {},
   "source": [
    "**BERT Pipeline**\n",
    "\n",
    "The pipeline that you will create follows a typical machine learning application pattern of pre-processing, training, evaluation, and model registration.\n",
    "\n",
    "In the processing step, you will perform feature engineering to transform the `review_body` text into BERT embeddings using the pre-trained BERT model and split the dataset into train, validation and test files. The transformed dataset is stored in a feature store. To optimize for Tensorflow training, the transformed dataset files are saved using the TFRecord format in Amazon S3.\n",
    "\n",
    "In the training step, you will fine-tune the BERT model to the customer reviews dataset and add a new classification layer to predict the `sentiment` for a given `review_body`.\n",
    "\n",
    "In the evaluation step, you will take the trained model and a test dataset as input, and produce a JSON file containing classification evaluation metrics.\n",
    "\n",
    "In the condition step, you will register the trained model if the accuracy of the model, as determined by our evaluation step, exceeds a given threshold value. \n",
    "\n",
    "![](bert_sagemaker_pipeline.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71da19-dd5b-42cd-bd13-5ebec3694407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "\n",
    "import os, sagemaker, logging, boto3, sagemaker, pandas as pd, json, botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "config = botocore.config.Config(user_agent_extra='dlai-pds/c2/w3')\n",
    "\n",
    "# low-level service client of the boto3 session\n",
    "sm = boto3.client(service_name='sagemaker', config=config)\n",
    "sm_runtime = boto3.client('sagemaker-runtime', config=config)\n",
    "sess = sagemaker.Session(sagemaker_client=sm, sagemaker_runtime_client=sm_runtime)\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "#Setup pipeline Name\n",
    "import time\n",
    "timestamp = int(time.time())\n",
    "pipeline_name = 'BERT-pipeline-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc035b-63bd-4f44-afa8-78c075d7b71b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure the dataset and processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518dda8-610f-4b8e-84ca-cbb699725185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIGURE PROCESSING STEP\n",
    "#Refer aws pipeline: https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html\n",
    "\n",
    "raw_input_data_s3_uri = 's3://dlai-practical-data-science/data/raw/'\n",
    "\n",
    "#For the pipeline workflow you will need to create workflow parameters of a specific type: integer, string, or float.\n",
    "from sagemaker.workflow.parameters import (ParameterInteger,ParameterString,ParameterFloat,)\n",
    "\n",
    "#set the parameters for the processing step.\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "train_split_percentage = ParameterFloat(name=\"TrainSplitPercentage\", default_value=0.90,)\n",
    "validation_split_percentage = ParameterFloat(name=\"ValidationSplitPercentage\", default_value=0.05,)\n",
    "test_split_percentage = ParameterFloat(name=\"TestSplitPercentage\", default_value=0.05,)\n",
    "balance_dataset = ParameterString(name=\"BalanceDataset\", default_value=\"True\",)\n",
    "max_seq_length = ParameterInteger(name=\"MaxSeqLength\", default_value=128,)\n",
    "feature_store_offline_prefix = ParameterString(name=\"FeatureStoreOfflinePrefix\", default_value=\"reviews-feature-store-\" + str(timestamp),)\n",
    "feature_group_name = ParameterString(name=\"FeatureGroupName\", default_value=\"reviews-feature-group-\" + str(timestamp))\n",
    "input_data = ParameterString(name=\"InputData\",default_value=raw_input_data_s3_uri,)\n",
    "\n",
    "#Setting up scikit-learn-based processor to construct a ProcessingStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "processor = SKLearnProcessor(framework_version='0.23-1',role=role,instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count, env={'AWS_DEFAULT_REGION': region},)\n",
    "\n",
    "#configure input channel for and output channel from processing step\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_inputs=[\n",
    "    ProcessingInput(input_name='raw-input-data', source=input_data,destination='/opt/ml/processing/input/data/', s3_data_distribution_type='ShardedByS3Key')]\n",
    "\n",
    "processing_outputs=[\n",
    "    ProcessingOutput(output_name='sentiment-train', source='/opt/ml/processing/output/sentiment/train', s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='sentiment-validation', source='/opt/ml/processing/output/sentiment/validation', s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='sentiment-test', source='/opt/ml/processing/output/sentiment/test', s3_upload_mode='EndOfJob')]        \n",
    "\n",
    "# similar to a processor instance's run method, use the processor instance to construct a ProcessingStep, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution\n",
    "processing_step = ProcessingStep(\n",
    "    name='Processing', code='./c2w3/src/prepare_data.py', processor=processor,inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=['--train-split-percentage', str(train_split_percentage.default_value),                   \n",
    "                   '--validation-split-percentage', str(validation_split_percentage.default_value),\n",
    "                   '--test-split-percentage', str(test_split_percentage.default_value),\n",
    "                   '--balance-dataset', str(balance_dataset.default_value),\n",
    "                   '--max-seq-length', str(max_seq_length.default_value),                   \n",
    "                   '--feature-store-offline-prefix', str(feature_store_offline_prefix.default_value),\n",
    "                   '--feature-group-name', str(feature_group_name.default_value)])        \n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6272474-f644-4124-a42f-c61aaeba1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the list of the processing job properties\n",
    "print(json.dumps(processing_step.properties.__dict__, indent=4, sort_keys=True, default=str))\n",
    "\n",
    "#Pull the channel sentiment-train from the output configuration of the processing job:\n",
    "print(json.dumps(processing_step.properties.ProcessingOutputConfig.Outputs['sentiment-train'].__dict__, \n",
    "                 indent=4, sort_keys=True, default=str))\n",
    "\n",
    "#print out attributes of the S3 output path related to the sentiment-train output channel:\n",
    "print(json.dumps(processing_step.properties.ProcessingOutputConfig.Outputs['sentiment-train'].S3Output.S3Uri.__dict__,\n",
    "                 indent=4, sort_keys=True, default=str))\n",
    "\n",
    "#These objects can be passed into the next steps of the workflow.\n",
    "\n",
    "#you can pull the arguments of the processing step with the corresponding function. The result is in the dictionary format\n",
    "processing_step.arguments.keys()\n",
    "\n",
    "#Pull and review processing inputs from the arguments of the processing step:\n",
    "processing_step.arguments['ProcessingInputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718abb25-dbd7-4d1c-a599-c7bb435c6deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19fd0168-325e-4efd-818e-bcf3d9e21d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>)\n"
     ]
    }
   ],
   "source": [
    "#Setup the parameters for the workflow\n",
    "freeze_bert_layer = ParameterString(name=\"FreezeBertLayer\",default_value=\"False\",)\n",
    "epochs = ParameterInteger(name=\"Epochs\", default_value=3)    \n",
    "learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.00001) \n",
    "train_batch_size = ParameterInteger(name=\"TrainBatchSize\", default_value=64)\n",
    "train_steps_per_epoch = ParameterInteger(name=\"TrainStepsPerEpoch\", default_value=50)\n",
    "validation_batch_size = ParameterInteger(name=\"ValidationBatchSize\", default_value=64)\n",
    "validation_steps_per_epoch = ParameterInteger(name=\"ValidationStepsPerEpoch\", default_value=50)\n",
    "seed = ParameterInteger(name=\"Seed\", default_value=42)\n",
    "run_validation = ParameterString(name=\"RunValidation\", default_value=\"True\",)\n",
    "train_instance_count = ParameterInteger(name=\"TrainInstanceCount\", default_value=1)\n",
    "train_instance_type = ParameterString(name=\"TrainInstanceType\", default_value=\"ml.c5.9xlarge\")\n",
    "train_volume_size = ParameterInteger(name=\"TrainVolumeSize\", default_value=256) \n",
    "input_mode = ParameterString(name=\"InputMode\", default_value=\"File\",)\n",
    "\n",
    "#Setup the dictionary that will be passed into the hyperparameters argument\n",
    "hyperparameters={'max_seq_length': max_seq_length, 'freeze_bert_layer': freeze_bert_layer,'epochs': epochs,\n",
    "    'learning_rate': learning_rate, 'train_batch_size': train_batch_size, \n",
    "    'train_steps_per_epoch': train_steps_per_epoch,'validation_batch_size': validation_batch_size, \n",
    "    'validation_steps_per_epoch': validation_steps_per_epoch,'seed': seed, 'run_validation': run_validation }\n",
    "\n",
    "#Configure model-evaluation metrics; Choose loss and accuracy as the evaluation metrics. For example, these sample log lines...\n",
    "metric_definitions = [{'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9.]+)'}, {'Name': 'validation:accuracy', 'Regex': 'val_acc: ([0-9.]+)'},]\n",
    "\n",
    "#CONFIGURE PYTORCH ESTIMATOR\n",
    "#A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later.\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator\n",
    "\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point='train.py',source_dir='c2w3/src', role=role,instance_count=train_instance_count, \n",
    "    instance_type=train_instance_type, volume_size=train_volume_size,\n",
    "    py_version='py3', framework_version='1.6.0', hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions, input_mode=input_mode)\n",
    "\n",
    "#Setup pipeline step caching\n",
    "#Step signature caching allows SageMaker Pipelines, before executing a step, to find a previous execution of a step that was called using the same arguments. Cache hit gets created if the previous execution is found. Then during execution instead of recomputing the step, pipelines propagates the values from the cache hit. details on SageMaker Pipeline step caching: https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\") # PT1H represents `one hour`\n",
    "\n",
    "#configure the TrainingStep calling the outputs of the processing step:\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Train',estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['sentiment-train'].S3Output.S3Uri,content_type='text/csv'),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs['sentiment-validation'].S3Output.S3Uri, content_type='text/csv')}, \n",
    "    cache_config=cache_config)\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13393bdd-58be-47cc-9e15-82961b239045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out attributes of the training step properties; The attributes match the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object.\n",
    "processing_step.properties.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175772d-4683-4973-9908-86cec98496e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='c2w3-3.'></a>\n",
    "## 3. Configure model-evaluation step\n",
    "\n",
    "First, develop an evaluation script that will be specified in the model evaluation processing step. The evaluation script users the trained model and the test dataset to produce a JSON file with classification evaluation metrics such as accuracy.\n",
    "\n",
    "After pipeline execution, you will examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script performs the following steps:\n",
    "* loads in the model\n",
    "* reads in the test data\n",
    "* issues a bunch of predictions against the test data\n",
    "* builds a classification report, including accuracy\n",
    "* saves the evaluation report to the evaluation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0530c2f-dc1b-451b-95f3-357340216606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the `SKLearnProcessor` to run our evaluation script as a scikit-learn-based SageMaker processing job.\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1', role=role, instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count, env={'AWS_DEFAULT_REGION': region}, max_runtime_in_seconds=7200)\n",
    "\n",
    "#Setup the output PropertyFile.\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "evaluation_report = PropertyFile(name='EvaluationReport', output_name='metrics', path='evaluation.json')\n",
    "\n",
    "#Use the processor instance to construct a ProcessingStep, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's run method.\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel', processor=evaluation_processor, code='c2w3/src/evaluate_model_metrics.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(source=training_step.properties.ModelArtifacts.S3ModelArtifacts, destination='/opt/ml/processing/input/model'),\n",
    "        ProcessingInput(source=processing_step.properties.ProcessingOutputConfig.Outputs['sentiment-test'].S3Output.S3Uri, destination='/opt/ml/processing/input/data')],\n",
    "    outputs=[ProcessingOutput(output_name='metrics', s3_upload_mode='EndOfJob', source='/opt/ml/processing/output/metrics/'),],\n",
    "    job_arguments=['--max-seq-length', str(max_seq_length.default_value),], \n",
    "    property_files=[evaluation_report],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ab550-a5ac-4ad6-8404-ef8941bc99cb",
   "metadata": {},
   "source": [
    "<a name='c2w3-4.'></a>\n",
    "# 4. Configure and register model step\n",
    "### 4.1. Configure the model for deployment\n",
    "\n",
    "Use the estimator instance that was used for the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a model package. A model package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. You can create a model package group for a specific ML business problem, and you can keep adding versions/model packages into it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker workflow pipeline so that they can keep adding versions/model packages to the group for every workflow pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is very similar to an estimator instance's `register` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, you will pass in the `S3ModelArtifacts` from the `training_step` properties.\n",
    "\n",
    "Of note, here you will be provided a specific model package group name which will be used in the Model Registry and Continuous Integration/Continuous Deployment (CI/CD) work later on. Let's setup the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1272f1c9-24ca-4164-98fc-7b1026a0dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Reviews-1658899925\n",
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f5962064350>\n",
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py36\n"
     ]
    }
   ],
   "source": [
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "deploy_instance_type = ParameterString(name=\"DeployInstanceType\", default_value=\"ml.m5.large\")\n",
    "deploy_instance_count = ParameterInteger(name=\"DeployInstanceCount\", default_value=1)\n",
    "\n",
    "model_package_group_name = f\"BERT-Reviews-{timestamp}\"\n",
    "print(model_package_group_name)\n",
    "\n",
    "#Configure the ModelMetrics to be stored as metadata.\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "\n",
    "model_metrics = ModelMetrics(model_statistics=MetricsSource(s3_uri=\"{}/evaluation.json\".format(\n",
    "                evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]),content_type=\"application/json\"))\n",
    "print(model_metrics)\n",
    "\n",
    "#Define deployment image for inference.\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(framework=\"pytorch\", region=region,version=\"1.6.0\", \n",
    "                                                    py_version=\"py36\", instance_type=deploy_instance_type, image_scope=\"inference\")\n",
    "print(inference_image_uri)\n",
    "\n",
    "#Register the model for deployment\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(name=\"RegisterModel\", estimator=estimator, image_uri=inference_image_uri, \n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts, content_types=[\"application/jsonlines\"],\n",
    "    response_types=[\"application/jsonlines\"], inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[deploy_instance_type], # batch transform is not used in this lab\n",
    "    model_package_group_name=model_package_group_name, approval_status=model_approval_status, model_metrics=model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab84ab-9dec-4f8a-8424-1c7249d281c8",
   "metadata": {},
   "source": [
    "<a name='c2w3-5.'></a>\n",
    "## 5. Create model for deployment step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daf7e4e2-a095-4600-8ca4-1d94bf208b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure model for deployment.\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'bert-model-{}'.format(timestamp)\n",
    "\n",
    "model = Model(name=model_name, image_uri=inference_image_uri,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts, sagemaker_session=sess, role=role,)\n",
    "\n",
    "#Now configure create model input:\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "create_inputs = CreateModelInput(instance_type=deploy_instance_type, )\n",
    "\n",
    "#Configure create model step for the workflow.\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "create_inputs = CreateModelInput(instance_type=deploy_instance_type, )\n",
    "\n",
    "#Configure create model step for the workflow\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "create_step = CreateModelStep(name=\"CreateModel\", model=model, inputs=create_inputs, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8775e7b-0bca-418a-8da8-85deb5f1631b",
   "metadata": {},
   "source": [
    "<a name='c2w3-6.'></a>\n",
    "# 6. Check accuracy condition step\n",
    "\n",
    "Finally, you would like to only register this model if the accuracy of the model, as determined by our evaluation step `evaluation_step`, exceeded some value. A `ConditionStep` allows for pipelines to support conditional execution in the pipeline DAG based on conditions of step properties. \n",
    "Below, you will:\n",
    "* define a minimum accuracy value as a parameter\n",
    "* define a `ConditionGreaterThan` on the accuracy value found in the output of the evaluation step, `evaluation_step`.\n",
    "* use the condition in the list of conditions in a `ConditionStep`\n",
    "* pass the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2d402a9-1574-43a0-95fe-d8ef53bfc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy_value = ParameterFloat(name=\"MinAccuracyValue\", default_value=0.33) # random choice from three classes\n",
    "\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (ConditionStep, JsonGet,)\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(step=evaluation_step, property_file=evaluation_report, json_path=\"metrics.accuracy.value\",),\n",
    "    right=min_accuracy_value) # minimum accuracy threshold\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step, create_step], # successfully exceeded or equaled the minimum accuracy, continue with model registration\n",
    "    else_steps=[],) # did not exceed the minimum accuracy, the model will not be registered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572a1de-337b-47df-8fd0-b4686cf1c6b4",
   "metadata": {},
   "source": [
    "<a name='c2w3-7.'></a>\n",
    "# 7. Create pipeline\n",
    "<a name='c2w3-7.1.'></a>\n",
    "### 7.1. Define a pipeline of parameters, steps, and conditions\n",
    "Let's tie it all up into a workflow pipeline so you can execute it, and even schedule it. A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair so you can append the timestamp to the name to reduce the chance of name conflict.\n",
    "\n",
    "Note:\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline need not be in the order of execution. The SageMaker workflow service will resolve the _data dependency_ DAG as steps the execution complete.\n",
    "* Steps must be unique to either pipeline step list or a single condition step if/else list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1475b4e-ec2f-4ae1-8bd9-627dc51b36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data, processing_instance_count, processing_instance_type, max_seq_length, balance_dataset,\n",
    "        train_split_percentage, validation_split_percentage, test_split_percentage, feature_store_offline_prefix,\n",
    "        feature_group_name, epochs, learning_rate, train_batch_size, train_steps_per_epoch, validation_batch_size,\n",
    "        validation_steps_per_epoch, freeze_bert_layer, seed, train_instance_count, train_instance_type, train_volume_size,        \n",
    "        input_mode, run_validation, min_accuracy_value, model_approval_status, deploy_instance_type, deploy_instance_count],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],sagemaker_session=sess,)\n",
    "\n",
    "#Let's examine the JSON of the pipeline definition that meets the SageMaker Workflow Pipeline DSL specification.By examining the definition, you are also confirming that the pipeline was well-defined, and that the parameters and step properties resolve correctly.\n",
    "import json\n",
    "from pprint import pprint\n",
    "definition = json.loads(pipeline.definition())\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "842b2350-7102-4d2d-b834-a2711aa3ce13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:170235698766:pipeline/bert-pipeline-1658899925\n",
      "arn:aws:sagemaker:us-east-1:170235698766:pipeline/bert-pipeline-1658899925/execution/6y432kk0ojbt\n"
     ]
    }
   ],
   "source": [
    "#Create pipeline\n",
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)\n",
    "\n",
    "#Start Pipeline: submit our pipeline definition to the Amazon SageMaker Pipeline service. \n",
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputData=raw_input_data_s3_uri, ProcessingInstanceCount=1, ProcessingInstanceType='ml.c5.2xlarge',\n",
    "        MaxSeqLength=128, BalanceDataset='True', TrainSplitPercentage=0.9, ValidationSplitPercentage=0.05,\n",
    "        TestSplitPercentage=0.05, FeatureStoreOfflinePrefix='reviews-feature-store-'+str(timestamp),\n",
    "        FeatureGroupName='reviews-feature-group-'+str(timestamp), Epochs=3, LearningRate=0.000012, TrainBatchSize=64,\n",
    "        TrainStepsPerEpoch=50, ValidationBatchSize=64, ValidationStepsPerEpoch=64, FreezeBertLayer='False', Seed=42,         \n",
    "        TrainInstanceCount=1, TrainInstanceType='ml.c5.9xlarge', TrainVolumeSize=256, InputMode='File', RunValidation='True',\n",
    "        MinAccuracyValue=0.01, ModelApprovalStatus='PendingManualApproval',  DeployInstanceType='ml.m5.large', DeployInstanceCount=1 ))\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c285f0-4638-4674-8777-5950350dd8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe execution instance and list the steps in the execution to find out more about the execution.\n",
    "from pprint import pprint\n",
    "execution_run = execution.describe()\n",
    "pipeline_execution_arn = execution_run['PipelineExecutionArn']\n",
    "execution_run_name = execution_run['PipelineExecutionDisplayName']\n",
    "pprint(execution_run_name)\n",
    "print(pipeline_execution_arn)\n",
    "print(execution_run)\n",
    "\n",
    "#Describe completed pipeline\n",
    "import time\n",
    "time.sleep(30)\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f48546-b78e-484f-b450-286f9aa86aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observing the pipeline execution summary and waiting for the execution status to change from Executing to Succeeded.\n",
    "#%%time\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)['PipelineExecutionSummaries']\n",
    "pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status=='Executing':\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)['PipelineExecutionSummaries']\n",
    "        pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "    except Exception as e:\n",
    "        print('Please wait...')\n",
    "        time.sleep(30)    \n",
    "    \n",
    "pprint(executions_response)\n",
    "\n",
    "pipeline_execution_status = executions_response[0]['PipelineExecutionStatus']\n",
    "print(pipeline_execution_status)\n",
    "pipeline_execution_arn = executions_response[0]['PipelineExecutionArn']\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da741eb-e0d1-4979-a87b-8c80f2ab1a07",
   "metadata": {},
   "source": [
    "<a name='c2w3-8.'></a>\n",
    "# 8. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ac6e2-196c-4540-9e76-276112903822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe evaluation metrics: Examine the resulting model evaluation after the pipeline completes. Download the resulting evaluation.json file from S3 and print the report.\n",
    "# pull and print the pipeline artifacts\n",
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "processing_job_name=None\n",
    "training_job_name=None\n",
    "\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    pprint(execution_step)\n",
    "    if execution_step['StepName'] == 'Processing':\n",
    "        processing_job_name=execution_step['Metadata']['ProcessingJob']['Arn'].split('/')[-1]\n",
    "        print('Processing job name: {}'.format(processing_job_name))\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step['StepName'] == 'Train':\n",
    "        training_job_name=execution_step['Metadata']['TrainingJob']['Arn'].split('/')[-1]\n",
    "        print('Training job name: {}'.format(training_job_name))\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)\n",
    "\n",
    "# list the files in the resulting output S3 path\n",
    "!aws s3 ls --recursive $transform_output_s3_uri\n",
    "\n",
    "#Download the evaluation report and print the accuracy.\n",
    "from pprint import pprint\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(evaluation_metrics_s3_uri))\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ea37b-88aa-4e47-93ab-ff0d0b429ee9",
   "metadata": {},
   "source": [
    "<a name='c2w3-9.'></a>\n",
    "# 9. Deploy and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f928146-2dff-46fa-9283-381e3313a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approve trained model: The pipeline created a model package version within the specified model package group and an approval status of `PendingManualApproval`.  This requires a separate step to manually approve the model before deploying to production. You can approve the model using the SageMaker Studio UI or programmatically as shown below.\n",
    "\n",
    "#Get the model package ARN\n",
    "for execution_step in execution.list_steps():\n",
    "    if execution_step['StepName'] == 'RegisterModel':\n",
    "        model_package_arn = execution_step['Metadata']['RegisterModel']['Arn']\n",
    "        break\n",
    "print(model_package_arn)\n",
    "\n",
    "#Update the model package with the Approved status to prepare for deployment\n",
    "model_package_update_response = sm.update_model_package(ModelPackageArn=model_package_arn, ModelApprovalStatus=\"Approved\",)\n",
    "pprint(model_package_update_response)\n",
    "\n",
    "#Get the model ARN and the model name from it.\n",
    "for execution_step in execution.list_steps():\n",
    "    print(execution_step['StepName'])\n",
    "    if execution_step['StepName'] == 'CreateModel':\n",
    "        model_arn = execution_step['Metadata']['Model']['Arn']\n",
    "        break\n",
    "print(model_arn)\n",
    "\n",
    "model_name = model_arn.split('/')[-1]\n",
    "print(model_name)\n",
    "\n",
    "#Configure the endpoint.\n",
    "endpoint_config_name = 'bert-model-epc-{}'.format(timestamp)\n",
    "print(endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{'InstanceType':'ml.m5.xlarge', 'InitialVariantWeight':1, 'InitialInstanceCount':1, 'ModelName': model_name, \n",
    "                         'VariantName':'AllTraffic'}])\n",
    "\n",
    "#Create the endpoint.\n",
    "pipeline_endpoint_name = 'bert-model-ep-{}'.format(timestamp)\n",
    "print(\"EndpointName={}\".format(pipeline_endpoint_name))\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(EndpointName=pipeline_endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "#review endpoint link\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST Endpoint</a></b>'.format(region, pipeline_endpoint_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aae5d7-947e-4e5b-b15f-fd0ef4651dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait till deployed\n",
    "%%time\n",
    "while True:\n",
    "    try: \n",
    "        waiter = sm.get_waiter('endpoint_in_service')\n",
    "        print('Waiting for endpoint to be in `InService`...')\n",
    "        waiter.wait(EndpointName=pipeline_endpoint_name)\n",
    "        break;\n",
    "    except:\n",
    "        print('Waiting for endpoint...')\n",
    "        endpoint_status = sm.describe_endpoint(EndpointName=pipeline_endpoint_name)['EndpointStatus']\n",
    "        print('Endpoint status: {}'.format(endpoint_status))\n",
    "        if endpoint_status == 'Failed':\n",
    "            break\n",
    "        time.sleep(30)  \n",
    "print('Endpoint deployed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ca23d-d776-47e5-8381-447c30f54fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model: Predict the sentiment with review_body samples and review the result:\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "inputs = [\n",
    "    {\"features\": [\"I love this product!\"]},\n",
    "    {\"features\": [\"OK, but not great.\"]},\n",
    "    {\"features\": [\"This is not the right product.\"]},]\n",
    "\n",
    "predictor = Predictor(endpoint_name=pipeline_endpoint_name, serializer=JSONLinesSerializer(), deserializer=JSONLinesDeserializer(),sagemaker_session=sess)\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for predicted_class in predicted_classes:\n",
    "    print(\"Predicted class {} with probability {}\".format(predicted_class['predicted_label'], predicted_class['probability']))\n",
    "    \n",
    "#SageMaker Studio provides a rich set of features to visually inspect SageMaker resources including pipelines, training jobs, and endpoints. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
