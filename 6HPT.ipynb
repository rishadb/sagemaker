{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d723f62e-0897-49c4-a261-231ff77882be",
   "metadata": {},
   "source": [
    "# Optimize models using Automatic Model Tuning\n",
    " In this lab you will apply a random algorithm of Automated Hyperparameter Tuning to train a BERT-based natural language processing (NLP) classifier. The model analyzes customer feedback and classifies the messages into positive (1), neutral (0), and negative (-1) sentiments.\n",
    " \n",
    " Amazon SageMaker supports Automated Hyperparameter Tuning. It runs multiple training jobs on the training dataset using the hyperparameter ranges specified by the user. Then it chooses the combination of hyperparameters that leads to the best model candidate. The choice is made based on the objective metrics, e.g. maximization of the validation accuracy. \n",
    "\n",
    "For the choice of hyperparameters combinations, SageMaker supports two different types of tuning strategies: random and Bayesian. This capability can be further extended by providing an implementation of a custom tuning strategy as a Docker container.\n",
    "\n",
    "<img src=\"c3w1/images/hpt.png\" width=\"70%\" align=\"center\"> \n",
    "\n",
    "In this lab you will perform the following three steps:\n",
    "\n",
    "<img src=\"c3w1/images/sagemaker_hpt.png\" width=\"50%\" align=\"center\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f803ccd-7b56-4d61-ba9e-5c947ca99d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#emli_notes: s3 bucket be in same region as training job, \n",
    "#for a project, create a bucket with the name of that project; use sm search to list all training jobs that had an s3 uri with this bucket whick tells the results of all previous jobs\n",
    "\n",
    "# please ignore warning messages during the installation\n",
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!conda install -q -y pytorch==1.6.0 -c pytorch\n",
    "!pip install --disable-pip-version-check -q transformers==3.5.1\n",
    "\n",
    "import boto3, sagemaker, pandas as pd, botocore\n",
    "\n",
    "config = botocore.config.Config(user_agent_extra='dlai-pds/c3/w1')\n",
    "\n",
    "# low-level service client of the boto3 session\n",
    "sm = boto3.client(service_name='sagemaker', config=config)\n",
    "\n",
    "sess = sagemaker.Session(sagemaker_client=sm)\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c1826-e984-4223-8676-51b1aa974248",
   "metadata": {},
   "source": [
    "<a name='c3w1-1.'></a>\n",
    "## 1. Configure dataset and Hyperparameter Tuning Job (HTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69926794-85ad-4aac-b5c1-ea42715b8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure dataset, Upload the data to the S3 bucket\n",
    "processed_train_data_s3_uri = 's3://{}/transformed/data/sentiment-train/'.format(bucket)\n",
    "processed_validation_data_s3_uri = 's3://{}/transformed/data/sentiment-validation/'.format(bucket)\n",
    "processed_test_data_s3_uri = 's3://{}/transformed/data/sentiment-test/'.format(bucket)\n",
    "!aws s3 cp --recursive ./data/sentiment-train $processed_train_data_s3_uri\n",
    "!aws s3 cp --recursive ./data/sentiment-validation $processed_validation_data_s3_uri\n",
    "!aws s3 cp --recursive ./data/sentiment-test $processed_test_data_s3_uri\n",
    "!aws s3 ls --recursive $processed_train_data_s3_uri\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "data_channels = {\n",
    "    'train': TrainingInput(s3_data=processed_train_data_s3_uri),\n",
    "    'validation':TrainingInput(s3_data = processed_validation_data_s3_uri)} #There is no need to create a test data channel, as the test data is used later at the evaluation stage and does not need to be wrapped into the sagemaker.inputs.TrainingInput function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e663a1-d53f-4a9f-bd42-288dfb3b6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure Hyperparameter Tuning Job\n",
    "\n",
    "#configure static hyperparameters\n",
    "max_seq_length=128 # maximum number of input tokens passed to BERT model\n",
    "freeze_bert_layer=False # specifies the depth of training within the network\n",
    "epochs=3; train_steps_per_epoch=50; validation_batch_size=64; validation_steps_per_epoch=50; seed=42\n",
    "\n",
    "train_instance_count=1; train_instance_type='ml.c5.9xlarge'; train_volume_size=256; input_mode='File'; run_validation=True\n",
    "\n",
    "#Some of these will be passed into the PyTorch estimator and tuner in the hyperparameters argument. Let's set up the dictionary for that:\n",
    "hyperparameters_static={\n",
    "    'freeze_bert_layer': freeze_bert_layer, 'max_seq_length': max_seq_length, 'epochs': epochs,\n",
    "    'train_steps_per_epoch': train_steps_per_epoch, 'validation_batch_size': validation_batch_size,\n",
    "    'validation_steps_per_epoch': validation_steps_per_epoch, 'seed': seed, 'run_validation': run_validation}\n",
    "\n",
    "#Configure hyperparameter ranges to explore in the Tuning Job. The values of the ranges typically come from prior experience, research papers, or other models similar to the task you are trying to do.\n",
    "from sagemaker.tuner import IntegerParameter; from sagemaker.tuner import ContinuousParameter; from sagemaker.tuner import CategoricalParameter\n",
    " \n",
    "    \n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.00005, scaling_type='Linear'), # specifying continuous variable type, the tuning job will explore the range of values\n",
    "    'train_batch_size': CategoricalParameter([128, 256]),} # specifying categorical variable type, the tuning job will explore only listed values, \n",
    "\n",
    "#Set up evaluation metrics: Choose loss and accuracy as the evaluation metrics. The regular expressions Regex will capture the values of metrics that the algorithm will emit.\n",
    "metric_definitions = [\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9.]+)'}, {'Name': 'validation:accuracy', 'Regex': 'val_acc: ([0-9.]+)'},]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9840516-310d-444d-b83c-28b400085c2f",
   "metadata": {},
   "source": [
    "## Run Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbfb74-1975-4925-a017-6b48c5d983a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the PyTorch model to run as a SageMaker Training Job:\n",
    "from sagemaker.pytorch import PyTorch as PyTorchEstimator # Note: indeed, it is not compulsory to rename the PyTorch estimator, but this is useful for code clarity, especially when a few modules of 'sagemaker.pytorch' are used\n",
    "\n",
    "estimator = PyTorchEstimator(\n",
    "    entry_point='train.py', source_dir='src', role=role, instance_count=train_instance_count,instance_type=train_instance_type, \n",
    "    volume_size=train_volume_size, py_version='py3', framework_version='1.6.0',\n",
    "    hyperparameters=hyperparameters_static, metric_definitions=metric_definitions,input_mode=input_mode,)\n",
    "\n",
    "#Launch the Hyperparameter Tuning Job: hyperparameter tuning search strategies: {https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html}\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator, hyperparameter_ranges=hyperparameter_ranges, metric_definitions=metric_definitions,\n",
    "    strategy=\"Random\",# other tuning strategies are bayesian, grid,..the selection of HP value ranges are based on this strategy\n",
    "    objective_type='Maximize', objective_metric_name='validation:accuracy',\n",
    "    max_jobs=2, # The max_parallel_jobs parameter limits the number of training jobs (and therefore hyperparameter combinations) to run in parallel within the tuning job. This parameter is often used in combination with the Bayesian search strategy when you want to test a smaller set of training jobs (less than the max_jobs), learn from the smaller set of training jobs, then apply Bayesian methods to determine the next set of hyperparameters used by the next set of training jobs. Bayesian methods can improve hyperparameter-tuning performance in some cases.\n",
    "    max_parallel_jobs=2, # maximum number of jobs to run in parallel\n",
    "    early_stopping_type='Auto') # refer: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html.;\n",
    "\n",
    "#Launch the SageMaker Hyper-Parameter Tuning (HPT) Job:\n",
    "tuner.fit(inputs=data_channels, # train and validation input\n",
    "          include_cls_metadata=False, # to be set as false if the algorithm cannot handle unknown hyperparameters\n",
    "          wait=False) # do not wait for the job to complete before continuing\n",
    "\n",
    "#Check Tuning Job status in link:\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name; print(tuning_job_name)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/hyper-tuning-jobs/{}\">Hyper-Parameter Tuning Job</a></b>'.format(region, tuning_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152de76-364c-459f-bb42-65ca67caf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait till job done:\n",
    "#%%time\n",
    "tuner.wait()\n",
    "#The results of the HPT are available on the analytics of the tuner object. The dataframe function converts the result directly into the dataframe. explore the results here:\n",
    "import time\n",
    "time.sleep(10) # slight delay to allow the analytics to be calculated\n",
    "\n",
    "df_results = tuner.analytics().dataframe()\n",
    "df_results.shape\n",
    "df_results.sort_values('FinalObjectiveValue', ascending=0)\n",
    "\n",
    "#When training and tuning at scale, it is important to continuously monitor and use the right compute resources. While you have the flexibility of choosing different compute options how do you choose the specific instance types and sizes to use? There is no standard answer for this. It comes down to understanding the workload and running empirical testing to determine the best compute resources to use for the training.\n",
    "#Training Jobs emit CloudWatch metrics for resource utilization in below link:\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review Training Jobs of the <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/hyper-tuning-jobs/{}\">Hyper-Parameter Tuning Job</a></b>'.format(region, tuning_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec37c01-f1d1-43e9-a5b0-239d5ff4fa7d",
   "metadata": {},
   "source": [
    "## 3. Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627ecb3-2b75-4652-be97-2bada42de07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the best candidate: can paste HPT job name to HPO_Analyze_Tuning_Job_results file in HPTuning from sagemaker exaples and analyze results too\n",
    "best_candidate = df_results.sort_values('FinalObjectiveValue', ascending=0).iloc[0]\n",
    "best_candidate_training_job_name = best_candidate['TrainingJobName']\n",
    "print('Best candidate Training Job name: {}'.format(best_candidate_training_job_name))\n",
    "best_candidate_accuracy = best_candidate[None] \n",
    "print('Best candidate accuracy result: {}'.format(best_candidate_accuracy))\n",
    "\n",
    "#use the function describe_training_job of the service client to get some more information about the best candidate. The result is in dictionary format. \n",
    "best_candidate_description = sm.describe_training_job(TrainingJobName=best_candidate_training_job_name)\n",
    "best_candidate_training_job_name2 = best_candidate_description['TrainingJobName']\n",
    "print('Training Job name: {}'.format(best_candidate_training_job_name2))\n",
    "\n",
    "#Pull the Tuning Job and Training Job Amazon Resource Name (ARN) from the best candidate training job description.\n",
    "best_candidate_tuning_job_arn = best_candidate_description[None] # Replace None\n",
    "best_candidate_training_job_arn = best_candidate_description[None] # Replace None\n",
    "print('Best candidate Tuning Job ARN: {}'.format(best_candidate_tuning_job_arn))\n",
    "print('Best candidate Training Job ARN: {}'.format(best_candidate_training_job_arn))\n",
    "\n",
    "#Pull the path of the best candidate model in the S3 bucket. need it later to set up the Processing Job for the evaluation.\n",
    "model_tar_s3_uri = sm.describe_training_job(TrainingJobName=best_candidate_training_job_name)['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_tar_s3_uri)\n",
    "\n",
    "#Evaluation with test dataset: To perform model evaluation, use a scikit-learn-based Processing Job.\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processing_instance_type = \"ml.c5.2xlarge\"\n",
    "processing_instance_count = 1\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",role=role,instance_type=processing_instance_type, \n",
    "    instance_count=processing_instance_count, max_runtime_in_seconds=7200,)\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor.run(\n",
    "    code=\"src/evaluate_model_metrics.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(input_name=\"model-tar-s3-uri\", source=model_tar_s3_uri, destination=\"/opt/ml/processing/input/model/\"),\n",
    "        ProcessingInput(input_name=\"evaluation-data-s3-uri\", source=processed_test_data_s3_uri, destination=\"/opt/ml/processing/input/data/\",),],\n",
    "    outputs=[ProcessingOutput(s3_upload_mode=\"EndOfJob\", output_name=\"metrics\", source=\"/opt/ml/processing/output/metrics\"),],\n",
    "    arguments=[\"--max-seq-length\", str(max_seq_length)],\n",
    "    logs=True, wait=False,)\n",
    "\n",
    "#pull the Processing Job name:\n",
    "scikit_processing_job_name = processor.jobs[-1].describe()[\"ProcessingJobName\"]\n",
    "print('Processing Job name: {}'.format(scikit_processing_job_name))\n",
    "\n",
    "#Pull the Processing Job status \n",
    "scikit_processing_job_status = processor.jobs[-1].describe()[None] \n",
    "print('Processing job status: {}'.format(scikit_processing_job_status))\n",
    "\n",
    "#Review the created Processing Job in the AWS console. \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67786a2d-7aa3-4b31-afe7-e2d3ab8731f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review the CloudWatch Logs.Wait for about 5 minutes \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> after about 5 minutes</b>'.format(region, scikit_processing_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e92f9-f39e-4ae9-b9fe-3c87f379717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the completion of the Processing Job you can also review the output in the S3 bucket\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 output data</a> after the Processing Job has completed</b>'.format(bucket, scikit_processing_job_name, region)))\n",
    "\n",
    "#Monitor the Processing Job:\n",
    "from pprint import pprint\n",
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name, sagemaker_session=sess)\n",
    "processing_job_description = running_processor.describe()\n",
    "pprint(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8bede-e639-481e-aa44-961bba22d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait for the Processing Job to complete.\n",
    "#%%time\n",
    "running_processor.wait(logs=False)\n",
    "\n",
    "#Inspect the processed output data: Get the S3 bucket location of the output metrics:\n",
    "processing_job_description = running_processor.describe()\n",
    "output_config = processing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"metrics\":\n",
    "        processed_metrics_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "print(processed_metrics_s3_uri)\n",
    "!aws s3 ls $processed_metrics_s3_uri/\n",
    "\n",
    "#The test accuracy can be pulled from the evaluation.json file:\n",
    "import json\n",
    "from pprint import pprint\n",
    "metrics_json = sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(processed_metrics_s3_uri))\n",
    "print('Test accuracy: {}'.format(json.loads(metrics_json)))\n",
    "\n",
    "#Copy image with the confusion matrix generated during the model evaluation into the folder generated.\n",
    "!aws s3 cp $processed_metrics_s3_uri/confusion_matrix.png ./generated/\n",
    "import time\n",
    "time.sleep(10) # Slight delay for our notebook to recognize the newly-downloaded file\n",
    "\n",
    "#Show and review the confusion matrix:\n",
    "#%%html\n",
    "<img src='./generated/confusion_matrix.png'>"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
